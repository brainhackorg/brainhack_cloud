[{"body":"Overview Once a new project is requested via the Issue Template one of the admins has to provision the project on the cloud.\nHere is an example of such a request:\nCreating a new user Login to Oracle Cloud: https://cloud.oracle.com/?region=eu-frankfurt-1\u0026tenant=brainhack\nOpen the menu and search for users, then open Users in Identity\nThen hit Create User.\nChange the selection to IAM User, add the user name as firstname_lastname and the same for description, and add an email.\nHit create.\nNow, generate a password for the user by Clicking Create/Reset Password\nCopy this password and send it to the user you just created.\nHere is an email template:\nDear You should have gotten an email for the Oracle account verification a few minutes ago ‚Äì check your spam folder if it‚Äôs not there üòä Your temporary password is: Your project compartment name is: When logging in (https://cloud.oracle.com/?region=eu-frankfurt-1\u0026tenant=brainhack) make sure to use ‚ÄúDirect Sign-In‚Äù (the SECOND sign-in option -\u003e NOT SSO). Let us know if you have any questions here: https://mattermost.brainhack.org/brainhack/channels/brainhack_cloud The new user has to follow this procedure: User request.\nAdd the User to the group projects (This group has policies for giving users access to the cloudshell and the data science notebooks):\nRepeat this procedure for every user in the project.\nCreate Project Group Go back to Identity and click Create Group\nGive the group a name that represents the project (no spaces!) - as a Description put the link to the Github issue.\nThen add the User(s) to the group.\nCreate Project Compartment Go back to Identity and head to Compartments and click Create Compartment. Name it like the group just created and add the Github issue link as the description. Parent compartment is projects a sub-compartment under the main compartment brainhack (root).\nCreate Policy for group and compartment Go back to Identity and click on Policies. Click Create Policy. Name the policy like the group and compartment just created. The description is the Github issue link. You can either use the policy builder or switch to manual. The resulting policy needs to be Allow group REPLACEWITHGROUPNAME to manage all-resources in compartment REPALCEWITHCOMPARTMENTNAME. Make sure that this policies is at the project level and not in the brainhack (root) compartment:\nCreate a Budget for compartment Budgets help us to control and monitor costs. For every compartment, we need a budget with someone being alerted when things go crazy:\nGo to Budgets under Cost Management and click Create Budget. Add the project details and add your email to the alert list.\n","categories":"","description":"Creating new users\n","excerpt":"Creating new users\n","ref":"/brainhack_cloud/admins/users/","tags":"","title":"Creating new users"},{"body":"Overview This section contains documentation on how to use the Oracle Cloud.\nFeedback \u0026 Inquiries To ask questions or suggest new features, join the discussion on Github. For issues with the brainhack cloud, please open a new issue.\nAcknowledgments Funding Thank you to Oracle for Research for providing Oracle Cloud credits and related resources to support this project.\nWhat is Oracle for Research? ‚ÄúOracle for Research supports academic, educational, and institutional researchers in all aspects of discovery. We‚Äôve made it simple for them to host, store and analyze their data with our cloud infrastructure solutions. Our research customers are changing the world ‚Äì and we‚Äôre proud to help them do it.‚Äù\nLicense CC BY License 4.0\n","categories":"","description":"Documentation.\n","excerpt":"Documentation.\n","ref":"/brainhack_cloud/docs/","tags":"","title":"Documentation"},{"body":"Request resources via a Github Issue Template Fill in this form.\nOnce an admin acts on this, you can continue.\nActivate your account An admin will create your account and send you a password and you will get an activation email from Oracle\nClick on the activation link.\nNow, it‚Äôs very important that you expand the LOWER section of this login field and fill in your Username from the email and the password you were sent!\nThen click on Sign In.\nThen change your password and hit Save New Password\nIf you are using any non Chrome, Safari or Firefox browser you need to indicate that you don‚Äôt want to be warned if things don‚Äôt work as expected (note: Edge works fine!).\nIf everything is complete you should see this.\nYou should now be able to select your Compartment (which should be named like your project) and start using resources.\nAlways make sure to select your compartment - otherwise you cannot see or create resources.\nHappy computing :)\n","categories":"","description":"Requesting Cloud Resources\n","excerpt":"Requesting Cloud Resources\n","ref":"/brainhack_cloud/docs/request/","tags":"","title":"Requesting Cloud Resources"},{"body":"Overview Running a Virtual machine on the Oracle cloud is the basis for many other things.\nSetup Make sure you selected the geographic region where you would like to create the resource.\nHere I create it in the Home Region, Frankfurt.\nNotice Ideally, you want the region to be as close to you (or your users) as possible to have low latencies. So, you should change the default from Frankfurt to another region from the list. Head to Compute -\u003e Instances\nCheck that you selected YOUR project compartment (testproject, is the example here - but you need to change this!) and click Create Instance\nSelecting an image and a shape You can name the instance and then select an Image (Oracle Linux is a good starting point as it has many tools installed that make it work very well on the Cloud) and select a Shape.\nVM.Standard.E4.Flex is a good starting point.\nWarning The default is Ampere and this is an ARM architecture, so most of your applications wouldn‚Äôt work out of the box - it is therefore important to change the default Shape to an x86 architecture, like the VM.Standard.E4.Flex. The network setup has sensible defaults to start with\nYou can either paste a public key you already have to access to this VM or create a key by choosing the option Generate a key pair for me under Add SSH Keys section.\nConnect to the VM using the Oracle key pair If you would like to use the key pairs the instance creation procedure generates, you can use the following steps based on your operating system.\nFirst download and install an SSH client to your machine (for Linux and Mac, use OpenSSH and for Windows use Putty).\nFrom the Instances dashboard, find your VM you would like to connect with ssh, and click to find and note its Public IP address.\nIf you are using Linux or Mac, run the following command on your local terminal to change the file permission on the private key file that you downloaded from the Oracle dashboard.\nWarning If you chose the option generate a key pair for me you need to change the permission on the file after downloading - otherwise it will result in a Permission denied (publickey,gssapi-keyex,gssapi-with-mic) error triggered by Load key \"*.key\": bad permissions chmod 600 /path/privateKeyFileName\nThen Run the below command to access the VM via SSH by pasting the IP address of the VM you created.\nssh opc@IPADDRESS -i /path/privateKeyFileName\nIf you are using Windows use the PuTTY Key Gen generator to generate a PuTTY Private Key file (.ppk).\nOpen the PuTTY Keygen Generator.\nLoad the downloaded private key file to the PuTTY Generator.\nEnter a phasephrase if you prefer to secure the private key to the Key passpphrase and Confirm passphrase fields, otherwise leave these as empty.\nThen click Save private key to save the private key file it produces.\nGo to your local command line (PowerShell) and connect to your VM with the below command\nputty -i C:\\Path\\privateKey.ppk opc@IPADDRESS.\nNow you are good to go!\nCreate your own public key If you don‚Äôt have a public key yet - this is how you can create one (for example in the cloudshell)\nOpen the Cloud Shell (this will take a few seconds the first time)\nRun ssh-keygen to create private/public key pairs (the defaults are sensible, so just hit Enter a few times)\nNow print the public key with cat ~/.ssh/id_rsa.pub and copy it to the clipboard.\nWarning Never share the private key with anyone, which is in id_rsa! Paste it in the Add SSH keys section\nDisk size You can specify a custom boot volume size, but you can also increase this later (described below).\nNote: it‚Äôs not possible to shrink a volume! Only increasing the size is possible, so start small and increase when needed. Increasing the size is even possible while the instance is running and will not interrupt your work :)\nThe rest of the defaults are sensible.\nCreate the VM This will now create the machine.\nConnect to Instance You can now use an SSH client on your computer to connect to the Instance, or the cloudshell.\nYou find the connection details in.\nSo in this case you would connect to your instance by typing.\nssh opc@130.61.212.59 If you are planning on running web services on the instance (like a jupyter notebook service) - then it‚Äôs easiest to connect to the instance via a port forwarding and then opening the web service in your local browser at localhost:portnumber.\nssh -L portnumber:127.0.0.1:portnumber opc@130.61.212.59 Accept the fingerprint and you should be connected.\nKeeping a process running even when disconnecting: For this you can use tmux: install tmux with:\nsudo yum install tmux then start a tmux session with:\ntmux then run whatever process or tool you need\nyou can disconnect from tmux by hitting CTRL-B and then d. Then you can disconnect from your SSH session (and this also happens when for example your internet connection gets disconnected).\nyou can reconnect to the tmux session using:\ntmux a Expand disk By default, the instance will not utilize the whole disk size!\nYou can check with df -h.\nBut it can expand the disk with the following commands.\nsudo dd iflag=direct if=/dev/oracleoci/oraclevda of=/dev/null count=1 echo \"1\" | sudo tee /sys/class/block/`readlink /dev/oracleoci/oraclevda | cut -d'/' -f 2`/device/rescan sudo /usr/libexec/oci-growfs -y If you are not using an Oracle Linux image, then oci-growfs will not be installed. You can create the script manually by copying this:\n#!/bin/bash # oci-utils # # Copyright (c) 2018, 2020 Oracle and/or its affiliates. All rights reserved. # Licensed under the Universal Permissive License v 1.0 as shown at http://oss.oracle.com/licenses/upl. ASSUME_YES=0 ASSUME_NO=0 # Print usage message usage() { cat \u003c\u003cEOF Usage: $0 [OPTION] Expand the root filesystem to its configured size. Option: -y Assume \"yes\" for all questions. -n Assume \"n\" to all questions (used for preview). -h Print this message. EOF } # Prompt for action confirmation confirm() { [ ${ASSUME_YES} -eq 1 ] \u0026\u0026 return 0 [ ${ASSUME_NO} -eq 1 ] \u0026\u0026 return 1 while true do # force use of a tty, if we are inside a 'read loop' already the prompt is never display and we loop forever read -p \"Confirm? [y/n]\" input \u003c /dev/tty case ${input} in [yY][eE][sS]|[yY]) return 0;; [nN][oO]|[nN]) return 1;; esac done } part_growfs_preview(){ if [ $# -ne 2 ]; then echo \"Invalid disk or partition.\" exit 1 fi growpart $1 $2 --dry-run return $? } part_growfs_func(){ if [ $# -ne 2 ]; then echo \"Invalid disk or partition.\" exit 1 fi growpart $1 $2 if [ $? -eq 0 ] then xfs_growfs / fi return $? } if [ \"$EUID\" -ne 0 ]; then echo \"This script needs root privileges to execute.\" exit 1 fi while [ \"$#\" -gt 0 ]; do case \"$1\" in -n|-N) ASSUME_NO=1 break ;; -y|-Y) ASSUME_YES=1 break ;; -h) usage exit 0 ;; -* | *) echo \"unknown option: $1\" \u003e\u00262; usage; exit 1 ;; esac done # first get storage type used for root FS _storage=`/usr/bin/findmnt --canonicalize --noheadings --output SOURCE /` # expecting lvm or part, /usr/bin/lsblk --noheadings -o TYPE,NAME $_storage | while read _type _sto do case \"${_type}\" in part) part_growfs_preview /dev/${_sto//[0-9]/} ${_sto//[^0-9]/} || exit 1 confirm if [ $? -eq 0 ] then part_growfs_func /dev/${_sto//[0-9]/} ${_sto//[^0-9]/} exit $? else exit 0 fi ;; lvm) # 1. find LV and VG of the device # we pipe to awk to strip away any leading space _root_vg=`/usr/sbin/lvs --noheadings --options vg_name --select lv_dm_path=$_storage | awk '{print $1}'` echo \"root VG: ${_root_vg}\" [ \"${_root_vg}\" == \"\" ] \u0026\u0026 echo \"Cannot find root volume group.\" \u0026\u0026 exit 1 # 2. find all PVs involve in the VG used for root for _pv in `/usr/sbin/pvs --noheadings --options pv_name --select vg_name=${_root_vg}` do # 3. find device of PVs _device=`/usr/sbin/pvs --noheadings --options devices --select pv_name=${_pv}` # device is suffixed with extne number like /dev/sda3(0) , just keep disk information parts _device=${_device//([0-9]*)/} # 3.1 extend the partittion part_growfs_preview ${_device//[0-9]/} ${_device//[^0-9]/} || exit 1 confirm if [ $? -eq 0 ] then echo \"calling part_growfs_func ${_device//[0-9]/} ${_device//[^0-9]/}\" part_growfs_func ${_device//[0-9]/} ${_device//[^0-9]/} [ $? != 0 ] \u0026\u0026 echo \"Cannot extend physical volume disk partition.\" \u0026\u0026 exit 1 else exit 0 fi # 3.1 extend the PV echo \"calling /usr/sbin/pvresize ${_pv}\" /usr/sbin/pvresize ${_pv} [ $? != 0 ] \u0026\u0026 echo \"Cannot extend physical volume.\" \u0026\u0026 exit 1 done # 4. extend the LV and the FS _lv_path=`/usr/sbin/lvs --noheadings --options lv_path --select lv_dm_path=$_storage` echo \"calling /usr/sbin/lvextend -l +100%FREE --resizefs ${_lv_path}\" /usr/sbin/lvextend -l +100%FREE --resizefs ${_lv_path} exit $? esac done exit $? into /usr/libexec/oci-growfs by doing this:\nsudo vi /usr/libexec/oci-growfs # hit i to activate insert mode, then paste, then hit SHIFT-Z-Z to exit and save sudo chmod a+x /usr/libexec/oci-growfs then execute the expansion command:\nsudo dd iflag=direct if=/dev/oracleoci/oraclevda of=/dev/null count=1 echo \"1\" | sudo tee /sys/class/block/`readlink /dev/oracleoci/oraclevda | cut -d'/' -f 2`/device/rescan sudo /usr/libexec/oci-growfs -y Now it‚Äôs using the full volume.\nIncrease disk size If you want to increase the disk space beyond the size that you initially set you need to first change the disk size in OCI. You can do this while the instance is running! Go to the instance and select Boot volume from the menu on the left:\nThen click on the boot volume name and click the Edit button:\nNow set the desired size and click Save Changes:\nThen login to the instance and expand the disk again to the new size:\nsudo dd iflag=direct if=/dev/oracleoci/oraclevda of=/dev/null count=1 echo \"1\" | sudo tee /sys/class/block/`readlink /dev/oracleoci/oraclevda | cut -d'/' -f 2`/device/rescan sudo /usr/libexec/oci-growfs -y Terminate the instance If you don‚Äôt need the machine anymore, you can Stop it (you don‚Äôt pay for the compute anymore, but the disk stays and you can start it back up later) or Terminate it (everything gets removed, including the boot volume if you want to):\nTo cleanup the storage as well, you can select Permanently delete the attached boot volume and click Terminate Instance.\nIf you aim to update the version of the Python that comes as default with VM or install environment you could install Miniconda by following below commands:\ncurl -o /tmp/miniconda.sh https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh; bash /tmp/miniconda.sh -b; miniconda3/bin/conda init this installation will give you all the necessary packages to start with building your own environment and project.\n","categories":"","description":"Virtual Machines\n","excerpt":"Virtual Machines\n","ref":"/brainhack_cloud/tutorials/vm/","tags":"","title":"Virtual machines"},{"body":"Overview Object Storage is like Dropbox ‚Ä¶ it allows you to simply store files and they can be accessible via links on the web (or APIs) and you don‚Äôt have to monitor the size of the disk (as opposed to block storage) and it comes with very nice features for mirroring your data to other regions or tiering data (making it cheaper if files are not accessed very often).\nSetup a new Bucket Select a Region from the list where you want your files to be located:\nThen search for Buckets and you will find it under Storage -\u003e Object Storage \u0026 Archival Storage -\u003e Buckets: Select your project‚Äôs compartment: Create a new Bucket:\ngive it a name In addition to the defaults we recommend Enable Auto-Tiering (this will make the storage cheaper by moving objects to lower tier storage if they are not used frequently) and Uncommitted Multipart Uploads Cleanup (this will clean up in case uploads failed halfway) Uploading files to a bucket You could upload files via the GUI in the Oracle cloud by clicking the Upload button: You could also use tools like rclone or curl or the OCI CLI to upload files (more about these tools later)\nMaking a Bucket public By default, the files in the bucket will not be visible to everyone. Let‚Äôs find the URL to the file we just uploaded: Click on the 3 dots next to the file and click on View Object details: When opening this URL, you will get this error: You can either make the WHOLE bucket visible to the world or use ‚ÄúPre-Authenticated Requests‚Äù. Let‚Äôs start with the easy (and less control/secure) way first:\nClick on Edit Visibility and switch to public: Now the file and EVERYTHING else in the bucket are visible to EVERYONE on the internet.\nPre-Authenticated Requests Click on the three dots next to the file again and Click Create Pre-Authenticated Request: This gives you more options to control access and you can also expire the access :)\nAnd you then get a specific URL to access the file (or the bucket or the files you configured): The URL will stop working when it expires or when you delete the Request. You find all requests under Pre-Authenticated Requests in the Resources menu: Tiering Lifecycle Rules allow you to control what happens with files after a certain amount of time. You can delete them or move them to Archival storage for example: Mirroring Mirroring allows you to keep the bucket up-to-date with another bucket in another region (e.g. main bucket is in Europe and the replica is in Australia). This is controlled under the Replication Policy.\nYou first need to create the target bucket in the other region and then you can configure it as a target: Mounting a Bucket inside a VM: First we need to install a few things on the VM (assuming Oracle Linux here):\nsudo rpm -Uvh https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm sudo yum update -y sudo yum install -y epel-release sudo yum install -y redhat-lsb-core sudo yum install -y s3fs-fuse Create a new bucket as described in the beginning of this page above.\nNext we need to create a Customer Secret Key. For this click on the user icon in the top right corner and select User settings.\nThen in the Resources menu on the left select ‚ÄúCustomer Secret Keys‚Äù and click Generate Secret Key. Give it a name and then copy the secret (it will never be shown again!). Then copy the Access Key shown in the table as well in a separate notepad.\nNow you need to store these two things inside ~/.passwd-s3fs:\necho FILL_IN_YOUR_ACCESS_KEY_HERE:FILL_IN_YOUR_SECRET_HERE \u003e ${HOME}/.passwd-s3fs and then you can mount the bucket your created earlier:\nchmod 600 ${HOME}/.passwd-s3fs sudo chmod +x /usr/bin/fusermount sudo mkdir /data sudo chmod a+rwx /data s3fs FILL_IN_YOUR_BUCKET_NAME /data/ -o endpoint=eu-frankfurt-1 -o passwd_file=${HOME}/.passwd-s3fs -o url=https://froi4niecnpv.compat.objectstorage.eu-frankfurt-1.oraclecloud.com/ -onomultipart -o use_path_request_style The bucket is now accessible under /data and you can almost treat it like a volume mount (but it‚Äôs not 100% posix complient).\nUploading files using CURL To enable this you need to create a Pre-Authenticated Request which allows access to the Bucket and it allows objects read and write and Object Listing: Then copy the URL, as it will never be shown again: Now you can use curl to upload files:\ncurl -v -X PUT --upload-file YOUR_FILE_HERE YOUR_PRE_AUTHENTICATED_REQUEST_URL_HERE Uploading files using RCLONE Rclone is a great tool for managing the remote file storages. To link it up with Oracles object storage you need to configure a few things (the full version is here: https://blogs.oracle.com/linux/post/using-rclone-to-copy-data-in-and-out-of-oracle-cloud-object-storage#:~:text=%20Using%20rclone%20to%20copy%20data%20in%20and,which%20Rclone%20will%20be%20used%20to...%20More%20):\nThe Amazon S3 Compatibility API relies on a signing key called a Customer Secret Key. You need to create this in your User‚Äôs settings: Then Click on Generate Secret Key under Customer Secret Keys: Save the Secret Key for later: Then save the Access Key from the table as well.\nFind out where your rclone config file is located: rclone config file Add this to your rclone config file: [myobjectstorage] type = s3 provider = Other env_auth = false access_key_id = \u003cACCESS KEY\u003e secret_access_key = \u003cSECRET KEY\u003e endpoint = froi4niecnpv.compat.objectstorage.\u003cREGION\u003e.oraclecloud.com Replace ACCESS KEY and SECRET KEY with the ones generated earlier. Replace REGION with the region where the storage bucket is located (e.g. eu-frankfurt-1).\nNow you can use rclone to for example list files in the bucket:\nrclone ls myobjectstorage:/test-bucket or you can upload files or whole directories (or download by reversing the order of Target/Source):\nrclone copy YOURFILE_or_YOURDIRECTORY myobjectstorage:/test-bucket or you can sync whole directories or other remote storage locations (includes deletes!):\nrclone sync YOURDIRECTORY_OR_YOUR_OTHER_RCLONE_STORAGE myobjectstorage:/test-bucket ","categories":"","description":"Object Storage\n","excerpt":"Object Storage\n","ref":"/brainhack_cloud/docs/object_storage/","tags":"","title":"Object Storage"},{"body":"Overview Access permissions are controlled via Policies. If something is not working, it‚Äôs usually a missing policy. One important thing to note is that Policies can ONLY be applied to Groups, NOT to the Users!\nPolicy for Cloud Shell access By default, a new user cannot open a Cloud Shell. To provide every user with this permission, we created a group called ‚Äúcloudshell-access‚Äù and we add every user to this group. The group then has a Policy that allows opening a cloud shell: allow group cloudshell-access to use cloud-shell in tenancy\nPolicies for Object Storage Lifecycle management To enable lifecycle management the local services need a policy to do this. I configured it for every possible region a user would choose:\nAllow service objectstorage-eu-frankfurt-1 to manage object-family in compartment projects\nAllow service objectstorage-ap-sydney-1 to manage object-family in compartment projects\nAllow service objectstorage-ap-melbourne-1 to manage object-family in compartment projects\nAllow service objectstorage-sa-saopaulo-1 to manage object-family in compartment projects\nAllow service objectstorage-sa-vinhedo-1 to manage object-family in compartment projects\nAllow service objectstorage-ca-montreal-1 to manage object-family in compartment projects\nAllow service objectstorage-ca-toronto-1 to manage object-family in compartment projects\nAllow service objectstorage-sa-santiago-1 to manage object-family in compartment projects\nAllow service objectstorage-eu-marseille-1 to manage object-family in compartment projects\nAllow service objectstorage-ap-hyderabad-1 to manage object-family in compartment projects\nAllow service objectstorage-ap-mumbai-1 to manage object-family in compartment projects\nAllow service objectstorage-il-jerusalem-1 to manage object-family in compartment projects\nAllow service objectstorage-eu-milan-1 to manage object-family in compartment projects\nAllow service objectstorage-ap-osaka-1 to manage object-family in compartment projects\nAllow service objectstorage-ap-tokyo-1 to manage object-family in compartment projects\nAllow service objectstorage-eu-amsterdam-1 to manage object-family in compartment projects\nAllow service objectstorage-me-jeddah-1 to manage object-family in compartment projects\nAllow service objectstorage-ap-singapore-1 to manage object-family in compartment projects\nAllow service objectstorage-af-johannesburg-1 to manage object-family in compartment projects\nAllow service objectstorage-ap-seoul-1 to manage object-family in compartment projects\nAllow service objectstorage-ap-chuncheon-1 to manage object-family in compartment projects\nAllow service objectstorage-eu-stockholm-1 to manage object-family in compartment projects\nAllow service objectstorage-eu-zurich-1 to manage object-family in compartment projects\nAllow service objectstorage-me-abudhabi-1 to manage object-family in compartment projects\nAllow service objectstorage-me-dubai-1 to manage object-family in compartment projects\nAllow service objectstorage-uk-london-1 to manage object-family in compartment projects\nAllow service objectstorage-uk-cardiff-1 to manage object-family in compartment projects\nAllow service objectstorage-us-ashburn-1 to manage object-family in compartment projects\nAllow service objectstorage-us-phoenix-1 to manage object-family in compartment projects\nAllow service objectstorage-us-sanjose-1 to manage object-family in compartment projects\nPolicies for HPC allow service compute_management to use tag-namespace in tenancy\nallow service compute_management to manage compute-management-family in tenancy\nallow service compute_management to read app-catalog-listing in tenancy\nAllow dynamic-group instance_principal to read app-catalog-listing in tenancy\nAllow dynamic-group instance_principal to use tag-namespace in tenancy\nAllow dynamic-group instance_principal to manage all-resources in compartment projects\n","categories":"","description":"This is about Oracle cloud policies.\n","excerpt":"This is about Oracle cloud policies.\n","ref":"/brainhack_cloud/admins/policies/","tags":"","title":"Setting up policies"},{"body":"Overview The Tenancy Explorer is very useful to see what resources are currently being used in your project.\nIt‚Äôs in Governance -\u003e Tenancy Explorer\nYou can use this to clean up resources that are no longer required.\nHint: The network setup creates a lot of things and to cleanup it‚Äôs easiest to go to Virtual Cloud Networks, select the VCN you don‚Äôt need anymore and click Terminate - this will go out and remove everything correctly:\n","categories":"","description":"Tenancy Explorer\n","excerpt":"Tenancy Explorer\n","ref":"/brainhack_cloud/docs/tenancy_explorer/","tags":"","title":"Tenancy Explorer"},{"body":"Overview The notebook service is like Google Colab, but without the time or resource limitations.\nStarting a notebook environment Select the geographic region where you want to run this (e.g. closest to you).\n!!! Important: If you want to use GPUs you need to select a region that has GPUs availabe!!!!\nNvidia GPUs V100 are available in Tokyo, London, (Seoul) Nvidia GPUs P100 are available in Frankfurt Nvidia GPUs V100 AND P100 are available in Ashburn CPU only instances are availabe in Sydney, Zurich, Stockholm, Singapore, Hyderabat, Marseille, Santiago, Toronto, Sao Paulo Then search for Data Science under Machine Learning then select your project compartment (in this example testproject) then click on create project: enter a Name and a Description then click Create notebook session Name the notebook session and select which resources you need: Set how much disk space you want under Block storage size (in GB), leave the Default networking and hit Create\nIt will now create everything for you: Once this is done (it will take 2-3minutes), you can open the notebook environment with a click on Open: Then you have to log in - leave Tenancy as brainhack and click Continue: and you have a fully configured notebook environment :) The notebook environment uses Oracle Linux as a base image, so if you want to install additional packages use:\nsudo yum update sudo yum install ... Hint for collaborating with multiple people: Multiple users can login to the same notebook system and work on separate notebooks simultaneously, but avoid editing the same notebook file - otherwise you risk overwriting your changes: Clean up for the day When completed for the day, you can save costs (especially important when using GPUs!) by deactivating the environment:\nClose the window and hit Deactivate This will shut down the compute instances but keep your data - so if you want to continue later, a click on Activate will bring everything back :) When reactivating you could even change the resources provided for the environment (e.g. adding a GPU or changing to a CPU only environment to save costs) :)\nClean up for good If you don‚Äôt need the notebook environment anymore you can delete everything (including the data) by More Actions -\u003e Delete\nA quick confirmation and a click on Delete will remove everything: ","categories":"","description":"Notebook service\n","excerpt":"Notebook service\n","ref":"/brainhack_cloud/tutorials/notebooks/","tags":"","title":"Notebooks"},{"body":"By default, every tenancy has strict resource limits. These make sure that the right resources are available in the data centers where they are requested. An overview of which resources are available where can be found here: https://www.oracle.com/cloud/data-regions/#northamerica\n","categories":"","description":"How resources work on the Oracle Cloud\n","excerpt":"How resources work on the Oracle Cloud\n","ref":"/brainhack_cloud/admins/resources/","tags":"","title":"Resources"},{"body":"We applied for funding for this project with Oracle Cloud for Research and the project is funded with $230,000.00 AUD from the 29th of January 2022 until 28th of January, 2024.\nFor help and support, please join the mattermost channel:\nhttps://mattermost.brainhack.org/brainhack/channels/brainhack_cloud https://mattermost.brainhack.org/brainhack/channels/ofr-support If you would like to join the admin team, please join this mattermost channel:\nhttps://mattermost.brainhack.org/brainhack/channels/brainhack_global-oracle The team behind the scenes:\nJohanna Bayer https://github.com/likeajumprope Remi Gau https://github.com/Remi-Gau Steffen Bollmann https://github.com/stebo85 Isil Bilgin https://github.com/complexbrains Samuel Guay https://github.com/SamGuay ","categories":"","description":"This is the team behind the scenes\n","excerpt":"This is the team behind the scenes\n","ref":"/brainhack_cloud/admins/team/","tags":"","title":"Team"},{"body":"Overview This section describes how to setup a Github action runner on the cloud that you can then use to run huge Github workflows that wouldn‚Äôt run in the hosted runners :)\nCreate a new VM or HPC See our VM or HPC Tutorials.\nConfigure Github Go to the repository settings and under Actions you will find Runners where you can add a self-hosted runner: After clicking on New self-hosted runner and selecting Linux you can copy and paste the first section to the VM created in Step 1: run the commands listed on that page in your VM and accept the defaults.\nTo keep the session running you can use either tmux:\nsudo install tmux tmux new -s runner ./run.sh CTRL-B d Or as an alternative start it as a service, because this will then survive restarts of the VM:\nsudo ./svc.sh install sudo ./svc.sh start Use custom runner in Action # Use this YAML in your workflow file for each job runs-on: self-hosted Here is an example: https://github.com/QSMxT/QSMxT/blob/master/.github/workflows/test_segmentation_pipeline.yml\nUsing cirun.io The next level to the above workflow is to create the runners on-demand instead of letting them run all the time. This will be more cost effective and it lets you run multiple CI workflows in parallel :). Cirun.io is a wonderful free service that lets us do this!\nFirst, sign up to cirun.io: https://cirun.io/auth/login\nSecond, make sure your brainhack cloud user account is in the group ‚Äúcirun‚Äù (open github issue or indicate this in your request)\nThen you need to link your cirun.io account to your github account and to the repository that should trigger the ciruns: Then you need to create an API key for cirun inside oracle cloud. Go to your User Settings (little user icon in the top right corner) and then ‚ÄúAPI Keys‚Äù. Then add an API Key, download the private key and copy the Configuration File Preview to cirun and add the compartment id:\n[DEFAULT] user=ocid1.user.oc1..aaaaaaaaj2poftoscirunfororaclecmpcbrmvvescirunfororacle4mtq \u003c\u003c\u003c\u003c\u003c\u003c FILL IN YOUR USER ID HERE! fingerprint=78:4c:99:1t:3d:1b:a8:ea:f2:dd:cr:01:5r:86:a2:84 \u003c\u003c\u003c\u003c FILL IN YOUR FINGERPRINT HERE! tenancy=ocid1.tenancy.oc1..aaaaaaaaydlj6wd4ldaamhmhcirunfororaclemocirunfororacle compartment_id=ocid1.compartment.oc1..aaaaaaaawc7zqarq7xddumdzuatu3bu3ir6ytlkgauyokgxtixj2y6szrd4q Then copy the private key inside the textbox on cirun and hit save.\nNow you need to create a .cirun.yml file at the top level of your github repository:\nrunners: - name: oracle-runner cloud: oracle instance_type: VM.Standard2.1 machine_image: ocid1.image.oc1.eu-frankfurt-1.aaaaaaaa6m57xzoztlide4653fjavkm6dpksmz3kaa4gig4h34jod76aapva region: eu-frankfurt-1 labels: - oracle and you need to change the ‚Äúruns-on‚Äù inside your github action from ‚Äúself-hosted‚Äù to ‚Äú[self-hosted, oracle]‚Äù\n","categories":"","description":"Github action runner\n","excerpt":"Github action runner\n","ref":"/brainhack_cloud/tutorials/gh_runner/","tags":"","title":"Github action runner"},{"body":"Overview Oracle cloud supports High Performance Computing and makes it very easy to setup your own HPC cluster in the cloud. This tutorial here is a basic introduction to get your started.\nThis is a tutorial about SLURM on OCI with more background information: SLURM on OCI tutorial\nYou can find an alternative setup (tailored at deep learning and GPUs here: GPU cluster)\nBefore you get started Consider if you actually need High Performance Computing (HPC) for your work. An HPC is a cluster consisting of multiple machines and it uses a head-node (here bastion host) from where jobs are submitted to this cluster using a job engine (for example slurm). If you have many jobs that need to be run independently than the setup described here will work well. A ‚Äúreal‚Äù HPC does more on top: There is a high-performance network between machines and it enables to run jobs that combine multiple machines (e.g. MPI). This would be needed if you have a problem that‚Äôs so large that a single machine wouldn‚Äôt be big enough. In this example here we build a cluster without this advanced networking. Most people will not need an HPC for their work and they should use a single virtual machine, because it requires considerably less setup work and easier to maintain.\nConfigure HPC cluster Download the Terraform configuration from here as a zip file: https://github.com/oracle-quickstart/oci-hpc/releases/tag/v2.9.2\nMake sure you selected the geographic region where you would like to create the resource (it should be close to you for best latencies). Then go to Stacks under Resource Manager: In the List Scope drop down menu, select your project compartment. Click Create Stack and upload the zip file as a Terraform configuration source.\ngive your cluster a name, but leave the default options for the rest:\nCheck that the cluster is being created in your compartment again and then hit Next\nIn cluster configuration you need to add your public SSH key for the opc admin account. Make sure to setup your SSH keys first create a public key\nIn Headnode options you need to select an Availability Domain. It doesn‚Äôt matter what you select there and the options will depend on the geographic region where you launch your HPC. You can keep the headnode default size, or you can select a different flavour:\nIn Compute node options you need to disable Use cluster network (this is for MPI and not required for most people. It requires special network hardware that‚Äôs not available in every region. If you need MPI please get in touch and we can help you setting this up). Select a compute node size that fits your problem size. Drop the initial compute size node to 1, because we will scale the cluster using autoscaling.\nIn Autoscaling you should enable scheduler based autoscaling, monitor the autoscaling and disable RDMA latency check if you are not using MPI.\nFor API authentication and Monitoring leave the defaults:\nFor Additional file system accept the defaults:\nFor Advanced bastion options, Avanced storage options and Network options you can accept the defaults:\nFor Software enable Install Spack package manager in addition to the defaults:\nThen hit next and on the next page scroll to the end and tick Run apply:\nThen hit Create\nThis will then create a custom HPC for your project (it will take a couple of minutes to complete).\nOnce everything is done you find the bastion IP (the ‚Äúhead node‚Äù or ‚Äúlogin node‚Äù) under Outputs: You can now ssh into the HPC as follows:\nssh opc@ipbastion Be aware that this ‚Äúopc‚Äù account is the admin account with sudo access of the cluster and should not be used to perform analyses. It is better to create a user account to perform the work in:\nOnce logged in with the opc account, you can create normal cluster users using the cluster command:\ncluster user add test These users can then login using a password only and do not require an SSH key.\nThere is a shared file storage (which can also be configured in size in the stack settings) in /nfs/cluster\nMore information can be found here: https://github.com/oracle-quickstart/oci-hpc\nConfiguring node memory When you first submit jobs using sbatch, if you followed the above setup you may find you recieve the following error:\nerror: Memory specification can not be satisfied This is happening as the RealMemory for each node (e.g. the amount of memory each compute node may use) has not yet been specified and defaults to a very low value. To rectify this, first work out how much memory to allocate to each node by running scontrol show nodes and looking at FreeMem. To change the RealMemory, you must edit the slurm configuration file (which may be found in /etc/slurm/slurm.conf). Inside the slurm configuration file you will find several lines which begin NodeName=. These specify the settings for each node. To fix the error, on each of these lines, add RealMemory=AMOUNT where AMOUNT is the amount of memory you wish to allow the node to use. Once you have done this, you must reconfigure slurm by running the following command:\nsudo scontrol reconfigure Configuring X11 forwarding If you want to use graphical aplications you need to install:\nsudo yum install install mesa-dri-drivers xorg-x11-server-Xorg xorg-x11-xauth xorg-x11-apps mesa-libGL xorg-x11-drv-nouveau.x86_64 -y sudo vi /etc/ssh/sshd_config change to:\nX11Forwarding yes X11UseLocalhost no then\nsudo systemctl restart sshd # or sudo service sshd restart For full functionality, you may also need to add PrologFlags=X11 to your /etc/slurm/slurm.conf, along with enabling the following additional parameters in you /etc/ssh/sshd_config:\nAllowAgentForwarding yes AllowTcpForwarding yes X11Forwarding yes X11DisplayOffset 10 X11UseLocalhost no On you main node, to restart slurm: ```console sudo slurmctld restart And on your worker nodes:\nsudo service slurmd restart After you‚Äôve updated slurm, you can confirm the Prolog setting has taken:\nsudo scontrol reconfigre; sudo scontrol show config | grep PrologFlags And also check that x11 works!\nsrun --x11 xeyes Troublehsooting: Editing a deployd stack fails This can have many reasons, but the first one to check is:\nError: 409-Conflict, The Instance Configuration ocid1.instanceconfiguration.oc1.phx.aaaaaaaabycbnzxq4uskt4f7mklp4g4fcqk4m42aabj2r2fkchjygppdudua is associated to one or more Instance Pools. This means that the Instance Pool blocks the terraform script. To get it back working you need to destroy the stack first and then rebuild it.\nAnother option is that the resource type you used is not supported:\nError: 400-InvalidParameter, Shape VM.Standard1.4 is incompatible with image ocid1.image.oc1..aaaaaaaamy4z6turov5otuvb3wlej2ipv3534agxcd7loajk2f54bfmlyhnq Suggestion: Please update the parameter(s) in the Terraform config as per error message Shape VM.Standard1.4 is incompatible with image ocid1.image.oc1..aaaaaaaamy4z6turov5otuvb3wlej2ipv3534agxcd7loajk2f54bfmlyhnq Here, I selected a shape that is too ‚Äúsmall‚Äù and it fails. It needs at least VM.Standard2.4\nInstalling Custom Software If you don‚Äôt want to use spack (or cannot) then a good strategy is to install under /nfs/cluster, add any relevant ‚Äúbin‚Äù directories it to your path, and install there. As an example we will install go:\n$ cd /nfs/cluster $ wget https://go.dev/dl/go1.19.linux-amd64.tar.gz $ sudo tar -C /nfs/cluster -xzf go1.19.linux-amd64.tar.gz $ rm go1.19.linux-amd64.tar.gz And then add the go bin to your bash profile (vim ~/.bash_profile) as follows:\nexport PATH=/nfs/cluster/go/bin:$PATH and when you open a new shell or source ~/.bash_profile you should be able to see go on your path:\n$ which go /nfs/cluster/go/bin/go $ go version go version go1.19 linux/amd64 Further, since it‚Äôs located in the /nfs/cluster directory, it will be available on other nodes! Here is how to see the other nodes you have:\n$ sinfo PARTITION AVAIL TIMELIMIT NODES STATE NODELIST compute* up infinite 1 idle compute-permanent-node-941 And then shell into one, and also find the go binary.\n$ ssh compute-permanent-node-941 Last login: Sun Aug 14 02:30:01 2022 from relative-flamingo-bastion.public.cluster.oraclevcn.com $ which go /nfs/cluster/go/bin/go Install Singularity First, system dependencies. Follow the example above in install custom software to install Go. Next, install Singularity dependencies. These will need to be installed to each node.\nsudo yum groupinstall -y 'Development Tools' sudo yum install libseccomp-devel squashfs-tools cryptsetup -y sudo yum install glib2-devel -y Ensure Go is on your path (as shown above). Then install Singularity. We will install from source.\nImportant ensure you don‚Äôt have anything (e.g., pkg-config) loaded from spack, as this can interfere with installing Singularity using system libs. Also note that installing with system libs is a workaround for spack singularity not working perfectly (due to setuid). This means you‚Äôll need to do these steps on each of your head login and worker nodes.\nYou can do the same with an official release. Note that you don‚Äôt need to compile this on the nfs node - you can compile it anywhere and make install to /nfs/cluster.\n$ git clone https://github.com/sylabs/singularity $ cd singularity $ git submodule update --init # You can also set prefix to be it's own directory, e.g., /nfs/cluster/singularity-\u003cversion\u003e $ ./mconfig --prefix=/nfs/cluster $ cd ./builddir $ make $ make install Once you install, make sure you add the newly created bin to your path (wherever that happens to be). E.g., that might look like:\nexport PATH=/nfs/cluster/go/bin:/nfs/cluster/bin:$PATH And then when you source your ~/.bash_profile you can test:\n$ which singularity /nfs/cluster/bin/singularity Advanced: Use MPI networking Your first need to request access to those resources with this form.\nThen follow the above instructions, but leave Use cluser network activated and RDMA options enabled.\n","categories":"","description":"High Performance Computing\n","excerpt":"High Performance Computing\n","ref":"/brainhack_cloud/tutorials/hpc/","tags":"","title":"HPC"},{"body":"Overview This section describes how to setup a Neurodesk instance on the cloud. Neurodesk provides a containerised data analysis environment to facilitate reproducible analysis of neuroimaging data. Software included can be found here: https://www.neurodesk.org/applications/\nCreate a new VM See our VM tutorial\nConfigure Docker/Podman See our Docker documentation\nFollow the official Neurodesk instructions for running the latest container https://www.neurodesk.org/docs/neurodesktop/getting-started/cloud/\n","categories":"","description":"Neurodesk\n","excerpt":"Neurodesk\n","ref":"/brainhack_cloud/tutorials/neurodesk/","tags":"","title":"Neurodesk"},{"body":"Overview To increase the security of our cloud tenancy an admin cannot create or remove other admins. This is the instruction for the admin-administrator to create new admin accounts:\nCreate a new admin Admins need to use a ‚Äúfederated account‚Äù to login into the cloud instead of ‚Äúlocal accounts‚Äù.\nGo to Federation in Identity Click on OracleIdentityCloudService and then Create User\nappend _admin to the Username (firstname_lastname) of the new user and add the user to the Group OCI_Administrators\nWarning: DO NOT ADD THE USER TO THE GROUP IDCS_Administrators - otherwise this admin will be able to manage other admin accounts.\nYou don‚Äôt have to assign any Roles to the user, so on the next screen click on Close.\nThe new admin will receive an email like this: and after activating the account everything is ready to go and the new admin needs to sign on via federated accounts Single Sign On: ","categories":"","description":"Creating new admins\n","excerpt":"Creating new admins\n","ref":"/brainhack_cloud/admins/admins/","tags":"","title":"Creating new admins"},{"body":"Overview This section describes how the Admin team manages users and projects on the Oracle Cloud.\nFeedback \u0026 Inquiries To ask questions or suggest new features, join the discussion on Github. For issues with the Brainhack Cloud, please open a new issue.\nFunding Thank you to Oracle for Research for providing Oracle Cloud credits and related resources to support this project.\nWhat is Oracle for Research? ‚ÄúOracle for Research supports academic, educational, and institutional researchers in all aspects of discovery. We‚Äôve made it simple for them to host, store and analyze their data with our cloud infrastructure solutions. Our research customers are changing the world ‚Äì and we‚Äôre proud to help them do it.‚Äù\nLicense CC BY License 4.0\n","categories":"","description":"Information for admins.\n","excerpt":"Information for admins.\n","ref":"/brainhack_cloud/admins/","tags":"","title":"Admins"},{"body":"Overview This section contains tutorials on how to setup various cool solutions on the cloud.\nFeedback \u0026 Inquiries To ask questions or suggest new features, join the discussion on github. For issues with the brainhack cloud, please open a new issue.\nAcknowledgments Funding Thank you to Oracle for Research for providing Oracle Cloud credits and related resources to support this project.\nWhat is Oracle for Research? ‚ÄúOracle for Research supports academic, educational, and institutional researchers in all aspects of discovery. We‚Äôve made it simple for them to host, store and analyze their data with our cloud infrastructure solutions. Our research customers are changing the world ‚Äì and we‚Äôre proud to help them do it.‚Äù\nLicense CC BY License 4.0\n","categories":"","description":"Tutorials\n","excerpt":"Tutorials\n","ref":"/brainhack_cloud/tutorials/","tags":"","title":"Tutorials"},{"body":"Step 1) Create VCN and Subnets Create a VCN and subnets using Virtual Cloud Networks \u003e Start VCN Wizard \u003e VCN with Internet Connectivity option. The Networking Quickstart option automatically creates the necessary private subnet with a NAT gateway. Step 2) Create a Dynamic Group Create a dynamic group with the following matching rule: ALL { resource.type = ‚Äòdatasciencenotebooksession‚Äô } created as ‚Äúdatascience-notebooks-dynamic-group‚Äù\nStep 3) Create Policies Create a policy in the root compartment with the following statements:\n3.1 Service Policies allow service datascience to use virtual-network-family in tenancy\n-\u003e created in root compartment as ‚Äúdatascience‚Äù\n3.2 Non-Administrator User Policies allow group data-scientists to use virtual-network-family in tenancy allow group data-scientists to manage data-science-family in tenancy where data-scientists represents the name of your user group -\u003e setup as ‚Äúprojects‚Äù group\n3.3 Dynamic Group Policies allow dynamic-group dynamic-group to manage data-science-family in tenancy where dynamic-group represents the name of your dynamic group\n-\u003e created as datascience-dynamic-group-policy in the root compartment\n","categories":"","description":"How to set up the datascience environments (only needs to be done once).","excerpt":"How to set up the datascience environments (only needs to be done ‚Ä¶","ref":"/brainhack_cloud/admins/datascience/","tags":"","title":"Datascience setup"},{"body":"Overview Docker (or a software container in general) is great for reproducibility and making it easy to move your tools in and out of the cloud. If you don‚Äôt know what containers are, here is a 3 minute explanation: https://www.youtube.com/watch?v=HelrQnm3v4g\nYou can either install the original ‚ÄúDocker‚Äù or a drop-in replacement called ‚ÄúPodman‚Äù\nInstalling Docker Docker is not installed by default on Oracle Linux and these steps will install and start Docker:\nsudo dnf install dnf-utils zip unzip sudo dnf config-manager --add-repo=https://download.docker.com/linux/centos/docker-ce.repo sudo dnf install docker-ce --nobest sudo systemctl enable docker.service sudo systemctl start docker.service OR: Installing Podman instead of Docker Podman is compatible with docker and is the default in Oracle Linux (and some argue it‚Äôs even better). This is how to install podman:\nsudo yum install docker Yes: This is actually installing podman in Oracle Linux!\nOr the direct way:\nsudo yum install podman If you installed podman using sudo yum install docker you can run docker commands directly, but it will tell you that this is actually podman:\ndocker Let‚Äôs remove that msg:\nsudo touch /etc/containers/nodocker Now we have podman installed as ‚Äúdocker‚Äù drop-in replacement and we can test it:\ndocker run hello-world and we could now run everything we like, e.g. https://neurodesk.github.io/docs/neurodesktop/getting-started/linux/\nmkdir /home/opc/neurodesktop-storage sudo yum install tmux tmux new -s neurodesk sudo docker run \\ --shm-size=1gb -it --privileged --name neurodesktop \\ -v ~/neurodesktop-storage:/neurodesktop-storage \\ -e HOST_UID=\"$(id -u)\" -e HOST_GID=\"$(id -g)\"\\ -p 8080:8080 \\ -h neurodesktop-20220302 docker.io/vnmd/neurodesktop:20220329 Hit CTRL-b and then d to detach from the tmux session (re-attaching is possible using tmux attach-session -t neurodesk).\nand this is how easy it is to run a container on the cloud :)\nif you connect to your cloud instance using a port-forwarding ssh -L 8080:127.0.0.1:8080 opc@xxx.xx.xx.xx then you could now use Neurodesktop via visiting http://localhost:8080/#/?username=user\u0026password=password in your local browser. When done, stop the container with CTRL-C and run sudo docker rm neurodesktop to cleanup.\n","categories":"","description":"Installing and using Docker ","excerpt":"Installing and using Docker ","ref":"/brainhack_cloud/docs/docker/","tags":"","title":"Installing and Using Docker"},{"body":"Overview Kubernetes enables running and orchestrating multiple software containers.\nSetup Kubernetes Cluster on Oracle Cloud using OKE Search for OKE in the menu and then go to Kubernetes Clusters (OKE) under Containers \u0026 Artifacts: Select the region where you would like to create the Cluster: Select the compartment where you would like to create the Cluster: Quick Create is great and gives a good starting point that works for most applications: The defaults are sensible, but Public Workers make it easier to troubleshoot things in the beginning: Select the shape you like and 1 worker is good to start (can be increased and changed later!) Under advanced you can configure the boot-volume size: Add an SSH key for troubleshooting worker nodes: Review everything and then hit Create and it should go and set-up everything :) That‚Äôs how easy it can be to setup a whole Kubernetes cluster :) Thanks OCI team for creating OKE! It will take a few minutes until everything is up and running -\u003e Coffee break?\nOnce everything is ready: you can access the cluster and the settings are given when clicking Access Cluster: Customizing nodes using Init-scripts If you configured Public IP addresses for the worker nodes, then you can connect to the nodes for troubleshooting - Click on the node under Nodes -\u003e pool1: By default the disks are NOT expanded to the Bootvolume size you configured, so this can be fixed via init scripts. Edit the node pool and under advanced set the inits script: This script will expand the disk:\n#!/bin/bash curl --fail -H \"Authorization: Bearer Oracle\" -L0 http://169.254.169.254/opc/v2/instance/metadata/oke_init_script | base64 --decode \u003e/var/run/oke-init.sh bash /var/run/oke-init.sh sudo dd iflag=direct if=/dev/oracleoci/oraclevda of=/dev/null count=1 echo \"1\" | sudo tee /sys/class/block/`readlink /dev/oracleoci/oraclevda | cut -d'/' -f 2`/device/rescan sudo /usr/libexec/oci-growfs -y Then hit Save Changes. To apply these configuration changes you need to Scale the pool to 0 and then backup to 1: Cleanup You can delete the whole cluster to cleanup: But be aware that Kubernetes can create resources via API calls, which is great, but it also means that these additionally created resources (like load balancers or storage volumes) will NOT be cleaned up automatically and need to cleaned up manually!\n","categories":"","description":"A description how to get started with Kubernetes on Oracle Cloud","excerpt":"A description how to get started with Kubernetes on Oracle Cloud","ref":"/brainhack_cloud/docs/kubernetes/","tags":"","title":"Kubernetes"},{"body":" Get started with Brainhack cloud Apply for Cloud Resources Tutorials Sign in to cloud What is Brainhack Cloud? A team of brainhack volunteers applied for Oracle Cloud Credits to support open source projects in and around brainhack with powerful cloud resources Read more ‚Ä¶\nDiscussions Ask questions or raise any issues you have Read more ‚Ä¶\nWhat is Oracle for Research? Oracle for Research supports academic, educational, and institutional researchers in all aspects of discovery. We've made it simple for them to host, store and analyze their data with our cloud infrastructure solutions. Our research customers are changing the world - and we're proud to help them do it. Read more ‚Ä¶\n","categories":"","description":"","excerpt":" Get started with Brainhack cloud Apply for Cloud Resources Tutorials ‚Ä¶","ref":"/brainhack_cloud/","tags":"","title":"Brainhack cloud"},{"body":"","categories":"","description":"","excerpt":"","ref":"/brainhack_cloud/search/","tags":"","title":"Search Results"}]